{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. NAIVE BAYES\n",
    "\n",
    "\n",
    "Contenido:\n",
    "1. Introducción y teoría de decisión bayesiana\n",
    "2. Probabilidad condicional y la suposición \"naive\"\n",
    "3. Clasificación con Naive Bayes (Gaussian, Multinomial, Bernoulli)\n",
    "4. Ejemplos ilustrativos (Iris, texto reducido con 20newsgroups)\n",
    "5. Regresión con una aproximación Naive (discretización)\n",
    "6. Implementación desde cero (Gaussian Naive Bayes, brute-force)\n",
    "7. Selección de modelo, validación y recomendaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción y teoría de decisión bayesiana\n",
    "\n",
    "La **teoría de la decisión bayesiana** proporciona un marco probabilístico para la clasificación: se asigna a una observación $x$ la clase $k$ que maximiza la probabilidad a posteriori $P(Y=k\\mid X=x)$, considerando una pérdida adecuada.\n",
    "\n",
    "La regla bayesiana de mínima probabilidad de error (0-1 loss) es:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}(x) = \\arg\\max_{k} P(Y=k \\mid X=x)\n",
    "\\end{equation}\n",
    "\n",
    "Por Bayes:\n",
    "\\begin{equation}\n",
    "P(Y=k \\mid X=x) = \\frac{P(X=x \\mid Y=k) P(Y=k)}{P(X=x)}.\n",
    "\\end{equation}\n",
    "\n",
    "Como $P(X=x)$ es común a todas las clases, la decisión se reduce a comparar $P(X=x\\mid Y=k)P(Y=k)$ entre clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Probabilidad condicional y la suposición \"naive\"\n",
    "\n",
    "El término *Naive* proviene de la suposición de **independencia condicional** entre las características dado $Y$:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X=x \\mid Y=k) = \\prod_{j=1}^d P(X_j = x_j \\mid Y=k).\n",
    "\\end{equation}\n",
    "\n",
    "Bajo esta suposición, el cálculo de la verosimilitud se simplifica enormemente y la regla de decisión se vuelve:\n",
    "\\begin{equation}\n",
    "\\hat{y}(x) = \\arg\\max_k P(Y=k) \\prod_{j=1}^d P(X_j = x_j \\mid Y=k).\n",
    "\\end{equation}\n",
    "\n",
    "Para evitar underflow numérico se usa la versión logarítmica (log-sum):\n",
    "\\begin{equation}\n",
    "\\hat{y}(x) = \\arg\\max_k \\left( \\log P(Y=k) + \\sum_{j=1}^d \\log P(X_j = x_j \\mid Y=k) \\right).\n",
    "\\end{equation}\n",
    "\n",
    "Dependiendo de la naturaleza de las variables $X_j$ elegimos distintos modelos para $P(X_j \\mid Y=k)$: Gaussiano para variables continuas, multinomial o Bernoulli para conteos/binarios, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clasificación con Naive Bayes — Formulaciones comunes\n",
    "\n",
    "- **Gaussian Naive Bayes (GNB):** assume $X_j\\mid Y=k \\sim \\mathcal{N}(\\mu_{jk}, \\sigma_{jk}^2)$. Entonces\n",
    "\\begin{equation}\n",
    "P(X=x \\mid Y=k) = \\prod_{j=1}^d \\frac{1}{\\sqrt{2\\pi\\sigma_{jk}^2}} \\exp\\left(-\\frac{(x_j-\\mu_{jk})^2}{2\\sigma_{jk}^2}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "- **Multinomial Naive Bayes:** usado para conteos de palabras, modela la probabilidad de la secuencia de conteos dados parámetros de tasa por clase.\n",
    "\n",
    "- **Bernoulli Naive Bayes:** variables binarias (presencia/ausencia de términos).\n",
    "\n",
    "En sklearn: `GaussianNB`, `MultinomialNB`, `BernoulliNB`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:33:33.402821Z",
     "start_time": "2025-11-05T22:33:31.806518Z"
    }
   },
   "source": [
    "# Ejemplo práctico: Gaussian Naive Bayes en Iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "print('Accuracy (GNB, Iris):', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (GNB, Iris): 0.9666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        11\n",
      "  versicolor       0.93      1.00      0.96        13\n",
      "   virginica       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.98      0.94      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones prácticas\n",
    "\n",
    "- GNB estima para cada característica y cada clase la media $\\mu_{jk}$ y varianza $\\sigma_{jk}^2$ (método de momentos / MLE bajo normalidad).\n",
    "- MultinomialNB requiere features no negativas (conteos o TF-IDF). BernoulliNB requiere binarias.\n",
    "- Naive Bayes es extremadamente rápido y robusto en alta dimensión cuando la independencia aproximada es razonable (por ejemplo, texto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ejemplo ilustrativo con texto (MultinomialNB)\n",
    "\n",
    "Usaremos `fetch_20newsgroups` pero restringiremos a dos categorías para mantenerlo ligero y claro: *sci.space* y *rec.sport.hockey* (clasificación binaria)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:34:01.841830Z",
     "start_time": "2025-11-05T22:33:33.415449Z"
    }
   },
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "categories = ['sci.space', 'rec.sport.hockey']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers','footers','quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers','footers','quotes'))\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english', max_df=0.7)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predicted = text_clf.predict(newsgroups_test.data)\n",
    "print('Accuracy (MultinomialNB, 20newsgroups subset):', accuracy_score(newsgroups_test.target, predicted))\n",
    "print(confusion_matrix(newsgroups_test.target, predicted))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (MultinomialNB, 20newsgroups subset): 0.9508196721311475\n",
      "[[391   8]\n",
      " [ 31 363]]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regresión con Naive Bayes — aproximación práctica (discretización)\n",
    "\n",
    "Naive Bayes es nativamente un clasificador. Para ilustrar una adaptación para problemas de regresión podemos **discretizar** la variable continua $y$ en bins y usar un clasificador (p. ej. MultinomialNB) para predecir el bin; la predicción continua se obtiene usando el valor promedio (o mediana) del bin predicho.\n",
    "\n",
    "Formalmente: definir partición $\\{B_1,\\dots,B_m\\}$ del rango de $Y$. Aprender $P(Y\\in B_r \\mid X=x)$ y predecir\n",
    "\\begin{equation}\n",
    "\\hat{y}(x) = E[Y \\mid X=x] \\approx \\sum_{r=1}^m m_r \\; P(Y\\in B_r \\mid X=x),\n",
    "\\end{equation}\n",
    "\n",
    "donde $m_r$ es el representante del bin $B_r$ (por ejemplo, su media). A continuación un ejemplo con `diabetes` discretizado."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:34:01.947631Z",
     "start_time": "2025-11-05T22:34:01.935702Z"
    }
   },
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "Xd, yd = load_diabetes(return_X_y=True)\n",
    "Xtr_d, Xte_d, ytr_d, yte_d = train_test_split(Xd, yd, test_size=0.3, random_state=0)\n",
    "\n",
    "# Discretizar y en m bins (ordinal)\n",
    "m = 10\n",
    "kbd = KBinsDiscretizer(n_bins=m, encode='ordinal', strategy='quantile')\n",
    "ytr_bins = kbd.fit_transform(ytr_d.reshape(-1,1)).ravel().astype(int)\n",
    "yte_bins = kbd.transform(yte_d.reshape(-1,1)).ravel().astype(int)\n",
    "\n",
    "# Entrenar GaussianNB sobre características continuas pero etiquetas discretas\n",
    "gnb_reg = GaussianNB()\n",
    "gnb_reg.fit(Xtr_d, ytr_bins)\n",
    "pred_bins = gnb_reg.predict(Xte_d)\n",
    "\n",
    "# Convertir bin predicho a valor real (media del bin en entrenamiento)\n",
    "bin_means = {b: ytr_d[ytr_bins==b].mean() for b in np.unique(ytr_bins)}\n",
    "y_pred_cont = np.array([bin_means[b] for b in pred_bins])\n",
    "print('Approx regression MSE via discretized NB:', mean_squared_error(yte_d, y_pred_cont))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx regression MSE via discretized NB: 4684.779436862775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:296: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementación de Gaussian Naive Bayes desde cero (brute-force)\n",
    "\n",
    "Implementamos estimadores MLE para $\\mu_{jk}, \\sigma_{jk}^2$ y la regla de decisión logarítmica para evitar underflow.\n",
    "\n",
    "Esta implementación sirve para fines pedagógicos y para comprobar la equivalencia con `sklearn.naive_bayes.GaussianNB` en pequeños datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:34:01.976030Z",
     "start_time": "2025-11-05T22:34:01.969327Z"
    }
   },
   "source": [
    "class GaussianNBBrute:\n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        self.classes_, counts = np.unique(y, return_counts=True)\n",
    "        self.class_prior_ = {c: cnt/len(y) for c, cnt in zip(self.classes_, counts)}\n",
    "        self.theta_ = {}  # mean per class and feature\n",
    "        self.sigma_ = {}  # var per class and feature\n",
    "        for c in self.classes_:\n",
    "            Xc = X[y==c]\n",
    "            self.theta_[c] = Xc.mean(axis=0)\n",
    "            # var MLE (biased) or unbiased? sklearn uses var with N, we use var with ddof=0\n",
    "            self.sigma_[c] = Xc.var(axis=0) + self.var_smoothing\n",
    "        return self\n",
    "    def _log_likelihood(self, x):\n",
    "        # returns dict class -> log p(x|class) + log prior\n",
    "        res = {}\n",
    "        for c in self.classes_:\n",
    "            mu = self.theta_[c]\n",
    "            var = self.sigma_[c]\n",
    "            # Gaussian log-likelihood (sum over features)\n",
    "            ll = -0.5 * np.sum(np.log(2 * np.pi * var)) - 0.5 * np.sum(((x - mu)**2) / var)\n",
    "            res[c] = ll + np.log(self.class_prior_[c])\n",
    "        return res\n",
    "    def predict(self, Xq):\n",
    "        Xq = np.atleast_2d(Xq)\n",
    "        preds = []\n",
    "        for x in Xq:\n",
    "            ll = self._log_likelihood(x)\n",
    "            preds.append(max(ll.items(), key=lambda kv: kv[1])[0])\n",
    "        return np.array(preds)\n",
    "\n",
    "# Comprobación con Iris (usar todo el dataset)\n",
    "clf_custom = GaussianNBBrute().fit(X_train, y_train)\n",
    "pred_custom = clf_custom.predict(X_test)\n",
    "print('Custom GNB accuracy on Iris test (brute):', accuracy_score(y_test, pred_custom))\n",
    "\n",
    "# Comparación con sklearn\n",
    "pred_sklearn = gnb.predict(X_test)\n",
    "print('Sklearn GNB accuracy on Iris test:', accuracy_score(y_test, pred_sklearn))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom GNB accuracy on Iris test (brute): 0.9666666666666667\n",
      "Sklearn GNB accuracy on Iris test: 0.9666666666666667\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Selección de variantes y validación\n",
    "\n",
    "- Use **GaussianNB** para features continuas que aproximen normalidad por clase.  \n",
    "- Use **MultinomialNB** para conteos / text data (previo `CountVectorizer` o `TfidfTransformer`).  \n",
    "- Use **BernoulliNB** para features binarias (p.ej. presencia/ausencia).  \n",
    "- Validar con *cross-validation* y calibrar prior si la clase está desbalanceada (parámetro `class_prior` en implementaciones).  \n",
    "- En problemas reales, la independencia condicional raramente es exacta; Naive Bayes suele funcionar bien por su parsimonia.\n",
    "\n",
    "### Recomendaciones finales\n",
    "- Reportar accuracy, precision/recall/F1 según el problema.  \n",
    "- Para texto: limpiar, eliminar stopwords, y considerar n-gramas con regularización al contar features.  \n",
    "- Para regresión via discretización: evaluar la granularidad de bins y la pérdida de información.\n",
    "\n",
    "### Referencias\n",
    "- Duda, R. O., Hart, P. E., & Stork, D. G. (2001). *Pattern Classification*.\n",
    "- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. (Capítulo sobre métodos generativos)\n",
    "- McCallum, A., & Nigam, K. (1998). A comparison of event models for naive bayes text classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
