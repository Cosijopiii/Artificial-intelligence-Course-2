{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDES NEURONALES ARTIFICIALES (RNA)\n",
    "\n",
    "## 1. Fundamentos de las RNA\n",
    "\n",
    "Las **Redes Neuronales Artificiales (RNA)** son modelos computacionales inspirados en el funcionamiento del cerebro humano. Su objetivo es aprender representaciones complejas a partir de datos mediante un conjunto de **nodos (neuronas)** interconectados organizados en capas.\n",
    "\n",
    "Formalmente, una red neuronal define una función:\n",
    "$$\n",
    "f(x; \\theta): \\mathbb{R}^d \\rightarrow \\mathbb{R}^k\n",
    "$$\n",
    "donde $x$ es el vector de entrada y $\\theta$ representa los parámetros (pesos y sesgos) aprendidos durante el entrenamiento.\n",
    "\n",
    "Cada neurona aplica una transformación lineal seguida de una función de activación no lineal:\n",
    "$$\n",
    "a = \\sigma(w^T x + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neurona Artificial\n",
    "\n",
    "Una **neurona artificial** es la unidad básica de procesamiento. Inspirada en las neuronas biológicas, recibe entradas, aplica pesos, suma los resultados y pasa el valor por una función de activación.\n",
    "\n",
    "### Modelo matemático:\n",
    "$$\n",
    "y = \\sigma\\left(\\sum_{i=1}^n w_i x_i + b\\right)\n",
    "$$\n",
    "donde:\n",
    "- $x_i$ son las entradas,\n",
    "- $w_i$ los pesos sinápticos,\n",
    "- $b$ es el sesgo,\n",
    "- $\\sigma(\\cdot)$ es la función de activación (sigmoide, ReLU, tanh, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ejemplo: Neurona artificial simple\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "x = np.array([0.5, 0.8, 0.2])\n",
    "w = np.array([0.4, -0.7, 0.1])\n",
    "b = 0.2\n",
    "\n",
    "y = sigmoid(np.dot(w, x) + b)\n",
    "print(f\"Salida de la neurona: {y:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Problemas manejados por las RNA\n",
    "\n",
    "Las redes neuronales se aplican en múltiples dominios:\n",
    "- **Clasificación:** reconocimiento de imágenes, detección de spam.\n",
    "- **Regresión:** predicción de precios, demanda o señales continuas.\n",
    "- **Series de tiempo:** predicción meteorológica, análisis financiero.\n",
    "- **Procesamiento de lenguaje natural:** traducción, chatbots.\n",
    "- **Visión por computadora:** segmentación, reconocimiento facial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Características Generales\n",
    "\n",
    "- Aprenden de los datos de forma no lineal.\n",
    "- Se adaptan a problemas complejos con muchas variables.\n",
    "- Requieren grandes volúmenes de datos y potencia computacional.\n",
    "- Son aproximadores universales: pueden aproximar cualquier función continua bajo ciertas condiciones (Teorema de Cybenko, 1989)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tipos de RNA\n",
    "\n",
    "| Tipo | Característica principal |\n",
    "|------|---------------------------|\n",
    "| **Perceptrón simple** | Red de una sola capa. |\n",
    "| **Multicapa (MLP)** | Varias capas con activaciones no lineales. |\n",
    "| **Convolucional (CNN)** | Procesamiento espacial, ideal para imágenes. |\n",
    "| **Recurrente (RNN, LSTM)** | Procesamiento secuencial o temporal. |\n",
    "| **Hopfield / Boltzmann** | Redes autoasociativas o de energía. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Redes Neuronales de un Nivel: Perceptrón\n",
    "\n",
    "### Representación del Perceptrón\n",
    "El perceptrón calcula:\n",
    "$$\n",
    "y = \\text{sign}(w^T x + b)\n",
    "$$\n",
    "donde $y \\in \\{-1, 1\\}$.\n",
    "\n",
    "### Entrenamiento del Perceptrón\n",
    "El algoritmo ajusta los pesos mediante actualización iterativa:\n",
    "$$\n",
    "w \\leftarrow w + \\eta (y_i - \\hat{y}_i) x_i\n",
    "$$\n",
    "donde $\\eta$ es la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ejemplo: Perceptrón para clasificación binaria\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Perceptron(eta0=0.1, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Redes de Hopfield\n",
    "\n",
    "### Modelo Básico\n",
    "Las **redes de Hopfield** son redes **recurrentes completamente conectadas** donde cada neurona está conectada a todas las demás. Se utilizan para el almacenamiento de patrones y la recuperación asociativa.\n",
    "\n",
    "La energía del sistema se define como:\n",
    "$$\n",
    "E = -\\frac{1}{2} \\sum_{i \\neq j} w_{ij} s_i s_j + \\sum_i \\theta_i s_i\n",
    "$$\n",
    "donde $s_i \\in \\{-1, 1\\}$ son los estados neuronales y $w_{ij}$ los pesos.\n",
    "\n",
    "### Entrenamiento\n",
    "Los pesos se calculan mediante la **regla de Hebb**:\n",
    "$$\n",
    "w_{ij} = \\frac{1}{N} \\sum_{p=1}^P s_i^p s_j^p\n",
    "$$\n",
    "donde $s_i^p$ es el estado de la neurona $i$ en el patrón $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Red Neuronal Multicapa (MLP)\n",
    "\n",
    "### Representación de la MLP\n",
    "Una MLP está compuesta por capas de neuronas donde la salida de una capa es la entrada de la siguiente:\n",
    "$$\n",
    "h^{(l)} = \\sigma(W^{(l)} h^{(l-1)} + b^{(l)})\n",
    "$$\n",
    "\n",
    "### Algoritmo de Retropropagación\n",
    "El entrenamiento se realiza minimizando una función de pérdida mediante **descenso del gradiente** y el **algoritmo de retropropagación (backpropagation)**.\n",
    "\n",
    "Actualización de pesos:\n",
    "$$\n",
    "W^{(l)} \\leftarrow W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ejemplo: MLP con sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', max_iter=300, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(f\"Accuracy: {mlp.score(X_test, y_test):.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Redes Neuronales Convolucionales (CNN)\n",
    "\n",
    "Las **CNN** son redes diseñadas para procesar datos con estructura espacial (imágenes, video). Emplean operaciones de **convolución** y **pooling** para extraer características locales y reducir dimensionalidad.\n",
    "\n",
    "### Convolución\n",
    "Operación matemática que combina una imagen con un filtro (kernel):\n",
    "$$\n",
    "(I * K)(x, y) = \\sum_m \\sum_n I(x-m, y-n) K(m, n)\n",
    "$$\n",
    "\n",
    "### Pooling\n",
    "Reduce la dimensionalidad manteniendo las características más importantes. Los tipos comunes son *max pooling* y *average pooling*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura LeNet (LeCun, 1998)\n",
    "Estructura clásica para reconocimiento de dígitos:\n",
    "1. Capas convolucionales con activaciones sigmoides.\n",
    "2. Capas de *pooling* (subsampling).\n",
    "3. Capas totalmente conectadas.\n",
    "\n",
    "### Arquitectura AlexNet (Krizhevsky et al., 2012)\n",
    "Profundizó el diseño de LeNet con:\n",
    "- Funciones de activación ReLU.\n",
    "- Normalización por lotes.\n",
    "- Dropout para evitar sobreajuste.\n",
    "- Entrenamiento en GPU."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Reproducibility and device\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data: MNIST with ToTensor (scales pixels to [0,1])\n",
    "transform = transforms.ToTensor()\n",
    "full_train = datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
    "\n",
    "# Train / validation split (90/10)\n",
    "train_len = int(0.9 * len(full_train))\n",
    "val_len = len(full_train) - train_len\n",
    "train_ds, val_ds = random_split(full_train, [train_len, val_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# CNN model similar to the Keras example\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 32, kernel_size=3)        # input 1x28x28 -> 32x26x26\n",
    "        self.pool = nn.MaxPool2d(2)                        # -> 32x13x13\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * 13 * 13, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)   # logits (CrossEntropyLoss expects raw logits)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training for one epoch with validation\n",
    "model.train()\n",
    "running_correct = 0\n",
    "running_total = 0\n",
    "for inputs, targets in train_loader:\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    preds = outputs.argmax(dim=1)\n",
    "    running_correct += (preds == targets).sum().item()\n",
    "    running_total += targets.size(0)\n",
    "\n",
    "train_acc = running_correct / running_total\n",
    "# Validation\n",
    "model.eval()\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        val_correct += (preds == targets).sum().item()\n",
    "        val_total += targets.size(0)\n",
    "\n",
    "val_acc = val_correct / val_total\n",
    "print(f\"Train accuracy: {train_acc:.3f} | Validation accuracy: {val_acc:.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Aplicación Social con RNA\n",
    "\n",
    "Como ejemplo de **impacto social**, una RNA puede usarse para detectar **enfermedades a partir de imágenes médicas**, **analizar desastres naturales** mediante visión satelital, o **predecir pobreza** usando datos socioeconómicos y geoespaciales.\n",
    "\n",
    "Ejemplo conceptual:\n",
    "- Datos: imágenes satelitales de viviendas.\n",
    "- Objetivo: predecir índice de bienestar.\n",
    "- Modelo: CNN entrenada sobre etiquetas socioeconómicas.\n",
    "- Beneficio: focalización de políticas públicas más efectivas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Bibliografía recomendada\n",
    "- Haykin, S. (1999). *Neural Networks: A Comprehensive Foundation*. Prentice Hall.\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.\n",
    "- LeCun, Y., Bengio, Y., & Hinton, G. (2015). *Deep learning*. Nature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
